import pandas as pd
import sqlite3
import numpy as np

# --- Configuration ---
DATABASE_FILE = "market_data.db" 
# ---------------------

def prepare_master_timeseries_data(db_file: str = DATABASE_FILE) -> pd.DataFrame:
    """
    Finalized function for Step 1: Data Preparation.
    This version includes robust handling for complex column names, 
    drops duplicate date entries, and performs a crucial check for 
    data quality (excessive NaN values).
    """
    
    # Define table names and the EXACT column name for the primary value
    table_map = {
        "reliance_stocks": {
            "final_col": "Reliance_Close", 
            "expected_cols": ["('stock_price', 'RELIANCE.NS')", 'stock_price', 'Close']
        }, 
        "gold_prices": {
            "final_col": "Gold_Price", 
            "expected_cols": ["('Gold Price (USD/oz)', 'GC=F')", 'Price', 'Rate']
        },
        "petrol_prices": {
            "final_col": "Petrol_Price", 
            "expected_cols": ["('oil_price', 'CL=F')", 'oil_price', 'Close']
        }, 
        "forex_rates": {
            "final_col": "USD_INR_Rate", 
            "expected_cols": ["('USDINR=X', 'USDINR=X')", "('EURINR=X', 'EURINR=X')", 'Rate', 'Price']
        }
    }
    
    all_dfs = []
    
    try:
        # A. LOAD AND INSPECT DATA
        with sqlite3.connect(db_file) as conn:
            print(f"âœ… Database connection successful: {db_file}")
            
            for table, mapping in table_map.items():
                print(f"\n Â  -> Loading table: {table}")
                
                # 1. Fetch all columns without setting an index yet
                df = pd.read_sql_query(f"SELECT * FROM {table}", conn)
                
                # ğŸ›‘ DEBUG: Columns loaded:
                # print(f" Â  ğŸ›‘ DEBUG: Columns loaded: {df.columns.tolist()}")
                
                # --- FIX 1: Robust Date/Index Column Identification & Duplicates Fix ---
                potential_date_names = ['date', 'index', 'level_0', df.columns[0].lower()]
                date_cols = [col for col in df.columns if col.lower() in potential_date_names]
                
                if not date_cols:
                    print(f" Â  [WARNING] Could not find a suitable Date/Index column in {table}. Skipping.")
                    continue
                
                date_col_name = date_cols[0]
                df = df.rename(columns={date_col_name: 'Date'})
                
                # VITAL FIX: Drop duplicates on the 'Date' column
                rows_before = len(df)
                df = df.drop_duplicates(subset=['Date'], keep='first')
                rows_after = len(df)
                print(f" Â  -> Rows dropped: {rows_before - rows_after} (Original: {rows_before}, Unique Dates: {rows_after})")

                df['Date'] = pd.to_datetime(df['Date'])
                df = df.set_index('Date')
                
                
                # --- FIX 2: Identify and Standardize the Value Columns ---
                found_value_cols = {} 

                # Handle Forex (two value columns)
                if table == 'forex_rates':
                    # Check for USDINR
                    usd_col = "('USDINR=X', 'USDINR=X')"
                    if usd_col in df.columns:
                        found_value_cols[usd_col] = 'USD_INR_Rate' 
                    # Check for EURINR
                    eur_col = "('EURINR=X', 'EURINR=X')"
                    if eur_col in df.columns:
                        found_value_cols[eur_col] = 'EUR_INR_Rate' 
                        
                # Handle Reliance, Gold, Petrol (one primary value column)
                else:
                    value_col = None
                    for expected_col in mapping['expected_cols']:
                        if expected_col in df.columns:
                            value_col = expected_col
                            found_value_cols[value_col] = mapping['final_col']
                            break
                
                if not found_value_cols:
                    print(f" Â  [WARNING] No primary value columns found in {table}. Skipping.")
                    continue
                    
                # Clean the DataFrame and prepare for merge
                df_cleaned = df[list(found_value_cols.keys())].rename(columns=found_value_cols)
                print(f" Â  -> Identified and renamed columns: {list(df_cleaned.columns)}")

                # *** CRITICAL VALIDATION CHECK for data quality ***
                # If a column is >90% NaN, it's corrupt and will cause the single-value ffill issue.
                for col in df_cleaned.columns:
                    nan_percentage = df_cleaned[col].isnull().sum() / len(df_cleaned)
                    if nan_percentage > 0.9:
                         print(f" Â  â— CRITICAL WARNING: Column '{col}' is {nan_percentage:.2%} NaN. The source data for this asset is likely corrupt or missing.")
                         print(" Â  Â  -> Expect single-value forward-fill (ffill) artifacts.")
                # *************************************************

                all_dfs.append(df_cleaned)

    except Exception as e:
        print(f"âŒ CRITICAL ERROR during database connection/loading: {e}")
        return None

    if not all_dfs:
        print("âŒ No valid dataframes were loaded. Cannot proceed.")
        return None

    # C. COMBINE TIME-SERIES DATA
    print("\n\n2. Combining and Cleaning Time-Series Data...")
    
    # Start with the first DataFrame
    master_df = all_dfs[0].copy()
    
    # Merge the rest using the common datetime index (outer merge)
    for df in all_dfs[1:]:
        master_df = master_df.merge(df, how='outer', left_index=True, right_index=True)

    print(f" Â  -> Combined Data Shape (before cleaning): {master_df.shape}")

    # B. HANDLE MISSING DATA (IMPUTATION)
    # 1. Reindex to include all dates between the min and max date (Daily frequency 'D')
    idx = pd.date_range(start=master_df.index.min(), end=master_df.index.max(), freq='D')
    master_df = master_df.reindex(idx)
    
    # 2. Forward-fill the missing values (fills weekends/holidays/missing market days)
    print(" Â  -> Imputing missing data using Forward-Fill (ffill)...")
    master_df = master_df.ffill()
    
    # Drop any rows that remain NaN (only dates before the earliest valid data point)
    master_df = master_df.dropna()
    
    print(f"\nâœ¨ FINAL MASTER DATAFRAME CREATED âœ¨")
    print(f" Â  -> Final Shape: {master_df.shape}")
    print(" Â  -> Columns:", master_df.columns.tolist())
    
    return master_df

# --- EXECUTE THE FUNCTION ---
if __name__ == "__main__":
    master_data = prepare_master_timeseries_data()
    
    if master_data is not None:
        print("\n--- Head of Master Data ---")
        print(master_data.head())
        print("\n--- Tail of Master Data ---")
        print(master_data.tail())